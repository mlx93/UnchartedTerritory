# PR1.6 AI SDK v5 Streaming & Body Parameter Research

**Date**: December 4, 2025
**Status**: Research Complete
**Related**: PR1.6_POST_IMPLEMENTATION_ISSUES.md

---

## Executive Summary

This research addresses the 5 questions from the post-implementation issues document. The key finding is that **the `body` parameter in `useChat` is captured at initialization and becomes stale** - this is the root cause of Issues #1 and #2.

---

## Question 1: Does AI SDK v5 `body` option get sent with every request?

### Answer: Yes, BUT values are frozen at initialization

The `body` parameter **is sent with every request**, but the values are **captured once when the hook initializes** and do not update with subsequent component re-renders.

### The Problem (Current Code)

```typescript
// client.tsx:69-89
const {
  messages,
  sendMessage,
  status,
} = useChat({
  api: "/api/chat",
  body: {
    provider: selectedProvider,
    model: selectedModel,
    workspaceId: workspace.id,      // Captured at first render
    revisionNumber,                  // Captured at first render
  },
});
```

**Issue**: If `workspace.id` or `revisionNumber` change (e.g., during navigation from `/test-ai-chat` landing page), the `useChat` hook still sends the **original values**.

### The Solution: Request-Level Body

Pass dynamic values at request time using the second argument to `sendMessage()`:

```typescript
const { sendMessage } = useChat({
  api: '/api/chat',
  // Static config only - no dynamic body params here
});

// When sending:
sendMessage(
  { text: input },
  {
    body: {
      provider: selectedProvider,
      model: selectedModel,
      workspaceId: workspace.id,     // Current value at request time
      revisionNumber,                // Current value at request time
    },
  },
);
```

### Alternative: useRef Pattern

```typescript
const workspaceIdRef = useRef(workspace.id);
const revisionNumberRef = useRef(revisionNumber);

useEffect(() => {
  workspaceIdRef.current = workspace.id;
  revisionNumberRef.current = revisionNumber;
}, [workspace.id, revisionNumber]);

const { sendMessage } = useChat({
  api: '/api/chat',
  body: () => ({
    workspaceId: workspaceIdRef.current,
    revisionNumber: revisionNumberRef.current,
  }),
});
```

### Source
- [AI SDK Troubleshooting: Stale Body Data](https://ai-sdk.dev/docs/troubleshooting/use-chat-stale-body-data)
- [AI SDK Custom Body Cookbook](https://ai-sdk.dev/cookbook/next/send-custom-body-from-use-chat)

---

## Question 2: Is there a race condition on component mount?

### Answer: No significant race - but first message has a different problem

The `useChat` hook **initializes synchronously**, so `sendMessage` is immediately available. There's no async initialization phase.

### Current Auto-Send Code (Correct Pattern)

```typescript
// client.tsx:114-123
useEffect(() => {
  const prompt = searchParams.get("prompt");
  if (prompt && !hasAutoSent && messages.length === 0) {
    setPendingPrompt(prompt);
    setHasAutoSent(true);
    sendMessage({ text: prompt });
  }
}, [searchParams, hasAutoSent, messages.length, sendMessage, workspace.id, revisionNumber]);
```

### The Actual Problem

The race condition is not in `sendMessage` availability - it's in the **stale body problem** from Question 1:

1. Component mounts with `workspace.id` = "abc123"
2. `useChat` initializes with body containing `workspaceId: "abc123"`
3. But if the workspace was just created (redirect from landing page), this value might not be stable yet
4. The `useEffect` fires `sendMessage()`
5. Request goes out with potentially incorrect/stale `workspaceId`

### Fix

Use request-level body in the auto-send:

```typescript
useEffect(() => {
  const prompt = searchParams.get("prompt");
  if (prompt && !hasAutoSent && messages.length === 0 && workspace?.id) {
    setPendingPrompt(prompt);
    setHasAutoSent(true);
    sendMessage(
      { text: prompt },
      {
        body: {
          provider: selectedProvider,
          model: selectedModel,
          workspaceId: workspace.id,     // Evaluated NOW
          revisionNumber,                // Evaluated NOW
        },
      }
    );
  }
}, [searchParams, hasAutoSent, messages.length, sendMessage, workspace?.id, revisionNumber]);
```

---

## Question 3: Correct pattern for dynamic params like workspaceId?

### Answer: Use request-level `body` in `sendMessage()` second argument

### Recommended Pattern

**Client Side:**
```typescript
'use client';
import { useChat } from '@ai-sdk/react';

export function ChatInterface({ workspaceId, revisionNumber }) {
  const { messages, sendMessage, status } = useChat({
    api: '/api/chat',
    // No body here - keep dynamic values out of hook-level config
  });

  const handleSubmit = (prompt: string) => {
    sendMessage(
      { text: prompt },
      {
        body: {
          workspaceId,        // Always current
          revisionNumber,     // Always current
          provider: 'anthropic',
          model: 'claude-sonnet-4-20250514',
        },
      }
    );
  };

  return /* UI */;
}
```

**Server Side (unchanged):**
```typescript
export async function POST(request: Request) {
  const { messages, workspaceId, revisionNumber } = await request.json();

  const tools = workspaceId
    ? createTools(authHeader, workspaceId, revisionNumber || 1)
    : undefined;

  const result = streamText({
    model: modelInstance,
    messages: convertToModelMessages(messages),
    tools,
  });

  return result.toUIMessageStreamResponse();
}
```

### What NOT to use
- **Headers** - for auth/metadata only
- **URL parameters** - less clean, harder to type
- **Hook-level body** - causes stale data

---

## Question 4: What transport/response combination works for tool support?

### Answer: Default transport + `toUIMessageStreamResponse()`

### Correct Pairing (Current Implementation is Correct!)

**Client:**
```typescript
const { messages, sendMessage } = useChat({
  api: '/api/chat',
  // No transport needed - DefaultChatTransport is the default
});
```

**Server:**
```typescript
const result = streamText({
  model: modelInstance,
  messages: convertToModelMessages(messages),
  tools,
  maxSteps: 5,
});

return result.toUIMessageStreamResponse();  // Correct for tools
```

### Why This Works

The UI Message Stream protocol (`toUIMessageStreamResponse()`) supports:
- Tool invocations (input streaming, execution, outputs)
- Multi-step LLM interactions
- Reasoning blocks
- File attachments
- Error handling

### What NOT to use

**`TextStreamChatTransport`** has limited functionality:
> "When using `TextStreamChatTransport`, **tool calls, usage information and finish reasons are not available**."

**Issue #3 was correctly identified and fixed** - removing `TextStreamChatTransport` was the right call.

### Source
- [AI SDK Stream Protocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol)
- [AI SDK Transport Documentation](https://ai-sdk.dev/docs/ai-sdk-ui/transport)

---

## Question 5: Does main workspace use Centrifugo or AI SDK streaming?

### Answer: Production uses Centrifugo ONLY - no AI SDK

### Architecture Comparison

| Aspect | Production (`/workspace/[id]`) | Test AI Chat (`/test-ai-chat`) |
|--------|-------------------------------|-------------------------------|
| **Real-time** | Centrifugo WebSocket | AI SDK SSE streaming |
| **Message creation** | `createChatMessageAction` → Go queue | `createAISDKChatMessageAction` → Direct DB |
| **AI processing** | Go worker via PostgreSQL queue | AI SDK `streamText` directly |
| **Tool execution** | Go backend (planned PR1.7) | AI SDK tools (implemented) |
| **File updates** | Centrifugo `artifact-updated` events | Manual refetch after tools |

### Production Flow

```
User message → createChatMessageAction
                      ↓
              Insert into workspace_chat
                      ↓
              enqueueWork("new_intent") → PostgreSQL queue
                      ↓
              Go worker picks up job
                      ↓
              Go LLM processes intent
                      ↓
              Publishes to Centrifugo
                      ↓
              Frontend receives chatmessage-updated event
                      ↓
              messagesAtom updated → UI re-renders
```

### Test AI Chat Flow

```
User message → createAISDKChatMessageAction (SKIPS Go queue)
                      ↓
              Insert into workspace_chat (is_intent_complete=true)
                      ↓
              sendMessage({ text })
                      ↓
              POST /api/chat with body params
                      ↓
              streamText with tools
                      ↓
              toUIMessageStreamResponse → SSE stream
                      ↓
              useChat hook updates messages
                      ↓
              React re-renders
```

### Key Insight

**They are intentionally separate systems**:
- Production bypasses `/api/chat` entirely
- Test AI Chat bypasses Go queue entirely
- `createAISDKChatMessageAction` specifically does NOT call `enqueueWork`

---

## Root Cause Analysis

### Issue #1: No Streaming on First Message

**Root Cause**: Stale `body` parameter + timing

When user lands on `/test-ai-chat/[workspaceId]?prompt=...`:
1. `useChat` initializes with `body` containing current `workspaceId`
2. `useEffect` fires `sendMessage()` immediately
3. BUT the body was captured before the effect ran
4. If there's any async state settling, values could be stale

**Fix**: Use request-level body in auto-send `useEffect`.

### Issue #2: Tool Hallucination

**Root Cause**: `workspaceId` is `undefined` in API request

When tools aren't created because `workspaceId` is falsy:
```typescript
// route.ts:113-115
const tools = workspaceId
  ? createTools(authHeader, workspaceId, revisionNumber || 1)
  : undefined;  // No tools = AI hallucinates XML
```

The model sees no tool schemas, so it "remembers" tool patterns from training and outputs fake XML.

**Fix**: Ensure `workspaceId` is always sent via request-level body.

### Issue #4: Conversation Order Reverses

**Likely Cause**: Database ORDER BY direction or message array concatenation order.

**Investigation needed** in:
- `getWorkspaceMessagesAction` - check ORDER BY clause
- `client.tsx` - check how `initialMessages` are rendered vs `messages` from `useChat`

### Issue #5: First Message vs Subsequent

**Root Cause**: Same as Issue #1 - stale body on first message, fresh on subsequent.

On subsequent messages:
- Component has been mounted for a while
- State has settled
- `sendMessage` is called with fresh values

On first message:
- Component just mounted
- `useEffect` fires immediately
- Body was captured during initialization

---

## Recommended Fix Implementation

### Step 1: Update `useChat` configuration

```typescript
// client.tsx - BEFORE
const {
  messages,
  sendMessage,
  status,
} = useChat({
  api: "/api/chat",
  body: {
    provider: selectedProvider,
    model: selectedModel,
    workspaceId: workspace.id,
    revisionNumber,
  },
});

// client.tsx - AFTER
const {
  messages,
  sendMessage,
  status,
} = useChat({
  api: "/api/chat",
  // Remove body from here - it will be passed per-request
  experimental_throttle: STREAMING_THROTTLE_MS,
  onError: (err) => console.error('[useChat] Error:', err),
  onFinish: () => console.log('[useChat] Finished'),
});
```

### Step 2: Create helper for body params

```typescript
// client.tsx - Add helper function
const getChatBody = useCallback(() => ({
  provider: selectedProvider,
  model: selectedModel,
  workspaceId: workspace.id,
  revisionNumber,
}), [selectedProvider, selectedModel, workspace.id, revisionNumber]);
```

### Step 3: Update auto-send useEffect

```typescript
// client.tsx - Update auto-send
useEffect(() => {
  const prompt = searchParams.get("prompt");
  if (prompt && !hasAutoSent && messages.length === 0 && workspace?.id) {
    console.log('[useChat] Auto-sending prompt from URL:', prompt);
    console.log('[useChat] Body will include workspaceId:', workspace.id);
    setPendingPrompt(prompt);
    setHasAutoSent(true);
    sendMessage(
      { text: prompt },
      { body: getChatBody() }  // Fresh values at request time
    );
  }
}, [searchParams, hasAutoSent, messages.length, sendMessage, workspace?.id, getChatBody]);
```

### Step 4: Update manual send

```typescript
// client.tsx - Update handleSendMessage
const handleSendMessage = async (e: React.FormEvent) => {
  e.preventDefault();
  if (!chatInput.trim() || isLoading) return;

  const messageText = chatInput.trim();
  setChatInput("");

  // Persist to DB
  const chatMessage = await createAISDKChatMessageAction(
    session,
    workspace.id,
    messageText
  );
  setCurrentChatMessageId(chatMessage.id);

  // Send to AI with fresh body params
  await sendMessage(
    { text: messageText },
    { body: getChatBody() }  // Fresh values at request time
  );
};
```

---

## Verification Steps

After implementing fixes:

1. **Check browser Network tab** for `/api/chat` POST request:
   - Verify `workspaceId` is in request body (not `null`/`undefined`)
   - Verify response is chunked (streaming)

2. **Check server terminal** logs:
   ```
   [/api/chat] Request received: { hasMessages: true, provider: 'anthropic', workspaceId: 'abc123', revisionNumber: 1 }
   [/api/chat] Tools created: { hasTools: true, toolNames: ['getChartContext', 'textEditor', 'latestSubchartVersion', 'latestKubernetesVersion'] }
   ```

3. **Test first message flow**:
   - Navigate to `/test-ai-chat`
   - Enter prompt, submit
   - Verify redirect shows loading state
   - Verify streaming response appears
   - Verify tools work (ask for subchart version)

---

## Related Documents

- `docs/issues/PR1.6_POST_IMPLEMENTATION_ISSUES.md` - Issue descriptions
- `docs/PR1.6_COMPLETION_REPORT.md` - What was implemented
- `PRDs/PR1.65_UI_PARITY_PLAN.md` - Blocked UI work

---

## Sources

- [useChat API Reference](https://ai-sdk.dev/docs/api-reference/use-chat)
- [Troubleshooting: Stale Body Values](https://ai-sdk.dev/docs/troubleshooting/use-chat-stale-body-data)
- [Custom Body Cookbook](https://ai-sdk.dev/cookbook/next/send-custom-body-from-use-chat)
- [Stream Protocol Documentation](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol)
- [Transport Documentation](https://ai-sdk.dev/docs/ai-sdk-ui/transport)
